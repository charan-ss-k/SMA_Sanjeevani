# LLM Provider Configuration
LLM_PROVIDER=ollama

# Ollama Settings (only used if LLM_PROVIDER=ollama)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=phi3.5

# LLM Parameters
# Lower temperature = more deterministic/consistent responses
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2048

# Note: Phi-3.5 is the FASTEST Ollama model (~2 seconds per response on most systems)
# Phi-3.5 is a 3.8 billion parameter model optimized for speed
# Available models: phi3.5 (fastest), mistral (balanced), neural-chat (slower but detailed)
