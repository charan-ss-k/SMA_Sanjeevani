# LLM Provider Configuration
# Change this to "ollama" to use Mistral-7B
# Keep as "mock" only for initial testing
LLM_PROVIDER=ollama

# Ollama Settings (only used if LLM_PROVIDER=ollama)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=neural-chat

# LLM Parameters
# Lower temperature = more deterministic/consistent responses
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=1024

# Note: First response may take 30-120 seconds depending on system
# Mistral-7B is a 7 billion parameter model running locally
